"\"\\nFooBar Transcript:\\n{\\n  \\\"message\\\": \\\"Hello world! And welcome to another episode of FooBar. I'm super excited. Finally, Bedrock is generally available. We can play with it. In this video, we are going to see what is Bedrock, very on top, because there is so many videos about it. But the interesting part that I want to show you is how you can connect Bedrock to your Lambda functions. So let's get started. So let's start by what is Bedrock. Amazon Bedrock has been announced in preview in April. And a few days ago, it has been made general available. That means that everybody can use it. However, it's only available in a few regions and with some limitations, we'll talk about those later. But now you can start building your applications. So the idea of Bedrock is that allows you to use foundational models for generative AI, totally managed. So it's the serverless generative AI service, and I'm so excited about it. I'm not a machine learning expert. So for me, SageMaker is cool, but it's too complicated. Bedrock is that thing I was looking for months. Now, We can build our applications and use Bedrock from an API perspective and get all the magic of generative AI in our applications. There is a few foundational models that are coming with Bedrock. So you can work with text and with images and more will be added in the future. So this is just the beginning. So let's go to the console and see a little bit of Bedrock in action. As I said, Bedrock is available in. Two regions is in North Virginia, us-east-1 and in it seems that now it's in Ohio, Oregon, Singapore, and nothing else. So let's go with Virginia because I'm there already and when you open from the console, you type Bedrock, it will take you here. So we can get started, but to get started with Bedrock, the first thing you need to do is to get access to the foundational models. So you are going to go to model access, and then here, you're going to select all the ones that you want to have access to, edit and boom, you select them all. So now I have Claude available and you will see that more models get available all the time. This means that now your account has access to those models based on IAM permission. So you just save the changes and that will now grant me access to cloud that I didn't have it before. You can see here, what is the different modality, text, embedding or images, and you can see from the different companies that are providing the models, what exact models you have available. So that's something to get started. After we get started, if you want, if you like to play with these, totally recommended, you can go to the chat and you can pick a model, for example, we can pick any model, and then you can start saying for example what is the color of the sky? I don't know. And it will answer something. The sky is typically blue. And why is the sky that color? And you have been using these chats forever. You know how this works. Then we have the image one, and I like this one a lot. It's using stable diffusion. And you can say give me an image that portrays a cat in the space, in space, wearing a bikini. I don't know. Let's see what it comes. And you can see that there is some inference configuration here in the side that you can tune. There is no bikini in this cat, but it's in space. And you can see how you can learn by clicking here. How you can tune. For example, if we want the input to have the prompt to have more strength, maybe this increasing this will get a better result of the bikini that the cat is wearing.Oh, now it's a kind of. I don't know if YouTube will like this image. So let's jump to the next one. And again, here you can see all the different models. You can see here that you can fine tune your own models, but I'm not getting there. That's out of my reach, but you can use any of the base models.So I will pick one of the models from here and I will say Something like, can you make a story about a cat in space wearing a bikini and its adventures With aliens. And again, here we have different configurations. These makes the randomness and the length of the response. So here you can see that the response is a trim. So we can increase these 2000 tokens. And this is important because different models have different tokens that they can handle. And you can see, for example, here that says Jumbo, Grande and Large support this amount of tokens and other models, this amount of tokens. So this is something you need to have in mind when working with these models as an API. You can see now that the story is a little bit longer. Once upon a time in a distant galaxy there was a god named Luna blah blah blah and it tells me all the story. You know how this works. Bedrock is very cool, but if you're watching this, you want to know how to use it. So Bedrock, as I said, it comes with support for embedding it into your applications as an API. So when you go to the documentation of Bedrock, you can see the different APIs operations that there are available for you. You can list all the models. You can get information about the models. You can run an inference. So this is the cool part. And there is two flavors of running the inference. The first one is to literally invoke the model in our request response manner. Or then you can invoke the model and get the response as a response stream. So if the model takes a while to return, you can start showing the response as it comes through. So I want to show you how you can invoke a model from a lambda function. Because that's what you're here for. So let's go to the code. And again, the code is available in GitHub for you to check it out if you want. We are going to use SAM but you could use CDK. You can use anything you want. Important things here. First, we are defining a function. This function, I will invoke it directly from the CLI. You can connect it to a API gateway if you want or whatever, but I want to show you the Bedrock integration. Second thing, until now, a Bedrock it's only available from boto3 so this means that  applications using Python can use AWS SDK that in Python word is called boto3, don't ask me why. So for the first time in this channel, we are going to create a Python Lambda function. You asked for it and here it comes. So that's the first thing. The second thing is that you need to give permissions to your lambda function to access bedrock. So that's something you can do, you can be more specific and say here that you want to invoke the model, but that's important. The third consideration is up to the time that I'm recording the version of AWS SDK that is inside the Lambda function. It's not yet the one that has support for Bedrock. So it's important that you add in a requirements. txt file and you put this version, you should check in your maybe you're watching this in the future. If you're a Lambda function already has a Boto version over this number, then you don't need to do this step. But because by the time I'm recording, the version that is in Lambda is 1. 27 something and doesn't include Bedrock. So this will fail. So that's the other consideration. Good. So now we have defined our function. Very simple with access to bedrock, we have make it with Python, and then we have created the we have imported the dependency of the boto on version that we want. The next step is to write the function. And the cool thing here is that this body that you see here is something that the Bedrock Playground will give you. So when you're working with an API in the Bedrock Playroom, you can. View API request. And you can see here, all these parameters that are here that you put to test that call in the playground, and then you can make that exactly same API request. To the model from your application. So when you're working with this, it's very easy to configure. So if you have select the amount of tokens or things like that, these all comes in these in these prompts. So you can see here, for example the tokens, you can see here the temperature, you can see here and. All the configurations for the penalty of repetitions and so on and so forth. You can see here the prompt. You can see here the name of the model that you will need here. So all of these is very important. And you can see that the four parts, body, model ID, accept, and content type that we have here are exactly as well in the call to the API. So this is really good because you don't need to start digging in the documentation on how to call this particular model, because that's another caveat. Every model has a little different way of making the body and you need to be aware of that. So even though we are using bedrock as a centralized place to connect to multiple models, every model is a tiny bit different and has tiny different parameters and names, and this. Will help you to make sure that you are calling the the model in the right way. So you can see here that this is the inference configuration, but if we go to to, let's see if I have another one here, there is different names. So it's temperature, PK, maximum tokens, top sequence. So the name is a little bit different and the API request in the body will be a little different. So that's something very important. When you're working with Bedrock, you cannot just change the model from A21 to Cohere or to Antropofit because it just doesn't work. So be careful. So in this function what we are going to do, the first thing is to get that client for Bedrock and here if you don't have the right version, It will fail. So how to know if you're using the latest version, you can just make a function that prints the version in the screen, and then you can add all the things if you're using a version over the 1. 2857, then you don't need the requirements file. If not, then you need it. And what this simple function does it has a prompt that says to write a compelling YouTube description for a video, maximum two paragraph with good SEO that don't focus on the person. I will basically pass in the prompt at a whole transcribe for a video. And then it should have the description in the same language as the video it is. So I can pass my English and my Spanish videos, and then I need to have five titles and 20 tags for this video. So everything is comma separated values, but then everything is formatted in JSON. It's very specific prompt, but that's the type of prompt we want to do in our application. So then I can use this response. Later on for doing something else. So this is how I build my prompt. And then I pass this text. That comes from the event object, and this is the transcribe from a video. I build a body and then I make an invoke to that particular model. I pass the body, I pass the model ID, I pass the accept and the content time. And then\""